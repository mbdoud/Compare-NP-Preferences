"""
This script uses `dms_tools` to infer amino-acid preferences for PR/1934 and Aichi/1968 NP, as well as take averages across replicate
preference measurements. Previously published amino-acid preference files for Aichi/1968 NP and WSN/1933 HA are also incorporated into 
the analysis pipeline. 

This master script has been tested on the FHCRC scientific computing cluster and uses sbatch to submit jobs. If you want to use the
current CPU, set `use_sbatch` in the main() function to False.

This script should be run with one option at the command line which specifies the path of a configuration file, eg:

python run_dmstools.py dmstoolsconfigfile.txt

Each line of the configuration file should have a key and a value, separated by a space, for the following keys:

mapmuts_output_dir: directory of mapmuts output generated by run_mapmuts.py, contains alignments and mutation counts files.
dmstools_output_dir: directory to save preferences files to.  will be generated if it doesn't already exist.
previous_aichi_analysis_dir: directory to previous mapmuts analysis of Aichi/1968 NP.
previous_ha_analysis_dir: directory to previous mapmuts analysis fo WSN/1933 HA.

This master script just runs scripts from ``dms_tools``; see the ``dms_tools`` documentation for more information
on the individual scripts used.

"""

import os
import sys
import string
import time
import multiprocessing
import glob
import mapmuts.io
import mapmuts.sequtils
import rmsdtools


def RunScript(rundir, run_name, script_name, commands, use_sbatch, sbatch_cpus, walltime=None, use_full_partition=True):
    """Runs a script with specified command-line options.

    This function was originally written by Jesse Bloom; I have modified it to 
    run scripts with command-line arguments (eg those taken by dms_tools) instead 
    of infiles (eg those taken by mapmuts). - Mike Doud

    *rundir* is the directory in which we run the job. Created if it does
    not exist.
    *run_name* is the name of the run, which should be a string without
    spaces.
    *script_name* is the name of the script that we run.
    *commands* is a list of commands for the script (command-line arguments).
    *use_sbatch* is a Boolean switch specifying whether we use ``sbatch``
    to run the script. If *False*, the script is just run with the command
    line instruction. If *True*, then ``sbatch`` is used, and the command file
    has the prefix *run_name* followed by the suffix ``.sbatch``.
    *sbatch_cpus* is an option that is only meaningful if *use_sbatch* is 
    *True*. It gives the integer number of CPUs that are claimed via
    ``sbatch`` using the option ``sbatch -c``. 
    *waltime* is an option that is only meaningful if *use_sbatch* is
    *True*. If so, it should be an integer giving the number of hours 
    to allocate for the job. If *walltime* has its default value of 
    *None*, no wall time for the job is specified.
    It is assumed that the script can be run at the command line using::
        script_name (command-line options)
    Returns *runfailed*: *True* if run failed, and *False* otherwise.
    """


    print "Running %s for %s in directory %s..." % (script_name, run_name, rundir)
    currdir = os.getcwd()
    if not os.path.isdir(rundir):
        os.mkdir(rundir)
    os.chdir(rundir)

    if (not run_name) or not all([x not in string.whitespace for x in run_name]):
        raise ValueError("Invalid run_name of %s" % run_name)

    if use_sbatch:
        sbatchfile = '%s.sbatch' % run_name # sbatch command file
        jobidfile = 'sbatch_%s_jobid' % run_name # holds sbatch job id
        jobstatusfile = 'sbatch_%s_jobstatus' % run_name # holds sbatch job status
        joberrorsfile = 'sbatch_%s_errors' % run_name # holds sbatch job errors
        sbatch_f = open(sbatchfile, 'w')
        sbatch_f.write('#!/bin/sh\n#SBATCH\n')
        if walltime:
            sbatch_f.write('#PBS -l walltime=%d:00:00\n' % walltime)


        sbatch_f.write('%s ' % (script_name))
        for command in commands:
            sbatch_f.write('%s ' % command)
        sbatch_f.close()

        if use_full_partition:
            os.system('sbatch -c %d -p full -e %s %s > %s' % (sbatch_cpus, joberrorsfile, sbatchfile, jobidfile))
        else:
            os.system('sbatch -c %d -e %s %s > %s' % (sbatch_cpus, joberrorsfile, sbatchfile, jobidfile))

        time.sleep(60) # short 1 minute delay
        jobid = int(open(jobidfile).read().split()[-1])
        nslurmfails = 0
        while True:
            time.sleep(60) # delay 1 minute
            returncode = os.system('squeue -j %d > %s' % (jobid, jobstatusfile))
            if returncode != 0:
                nslurmfails += 1
                if nslurmfails > 180: # error is squeue fails at least 180 consecutive times
                    raise ValueError("squeue is continually failing, which means that slurm is not working on your system. Note that although this script has crashed, many of the jobs submitted via slurm may still be running. You'll want to monitor (squeue) or kill them (scancel) -- unfortunately you can't do that until slurm starts working again.")
                continue # we got an error while trying to run squeue
            nslurmfails = 0
            lines = open(jobstatusfile).readlines()
            if len(lines) < 2:
                break # no longer in slurm queue
        errors = open(joberrorsfile).read().strip()
    

    else: 
        full_command = '%s ' % (script_name) + ' '.join(commands)
        errors = os.system(full_command)


    os.chdir(currdir)
    if errors:
        print "ERROR running %s for %s in directory %s." % (script_name, run_name, rundir)
        return True
    else:
        print "Successfully completed running %s for %s in directory %s." % (script_name, run_name, rundir)
        return False


def RunProcesses(processes, nmultiruns):
    """Runs a list *multiprocessing.Process* processes.
    *processes* is a list of *multiprocessing.Process* objects that
    have not yet been started.
    *nmultiruns* is an integer >= 1 indicating the number of simultaneous
    processes to run.
    Runs the processes in *processes*, making sure to never have more than
    *nmultiruns* running at a time. If any of the processes fail (return
    an exitcode with a boolean value other than *False*), an exception
    is raised immediately. Otherwise, this function finishes when all
    processes have completed.
    """
    if not (nmultiruns >= 1 and isinstance(nmultiruns, int)):
        raise ValueError("nmultiruns must be an integer >= 1")
    processes_started = [False] * len(processes)
    processes_running = [False] * len(processes)
    processes_finished = [False] * len(processes)
    while not all(processes_finished):
        if (processes_running.count(True) < nmultiruns) and not all(processes_started):
            i = processes_started.index(False)
            processes[i].start()
            processes_started[i] = True
            processes_running[i] = True
        for i in range(len(processes)):
            if processes_running[i]:
                if not processes[i].is_alive():
                    processes_running[i] = False
                    processes_finished[i] = True
                    if processes[i].exitcode:
                        raise IOError("One of the processes failed to complete.")
        time.sleep(60)




def main():

    configfilename = sys.argv[1]
    if not os.path.isfile(configfilename):
        raise IOError("Failed to find configuration file %s" % configfilename)
    d = mapmuts.io.ParseInfile(open(configfilename))


    mapmuts_output_dir = mapmuts.io.ParseStringValue(d, 'mapmuts_output_dir')
    print "Using mapmuts output directory %s" % mapmuts_output_dir
    dmstools_output_dir = mapmuts.io.ParseStringValue(d, 'dmstools_output_dir')
    print "Using dmstools output directory %s" % dmstools_output_dir
    previous_aichi_analysis_dir = mapmuts.io.ParseStringValue(d, 'previous_aichi_analysis_dir')
    print "Using previous Aichi/1968 analysis directory %s" % previous_aichi_analysis_dir
    previous_ha_analysis_dir = mapmuts.io.ParseStringValue(d, 'previous_ha_analysis_dir')
    print "Using previous WSN-HA analysis directory %s" % previous_ha_analysis_dir

    if not os.path.isdir(dmstools_output_dir):
        os.mkdir(dmstools_output_dir)

    # set to False to use current CPU, keep True to use sbatch to submit jobs:
    use_sbatch = True

    # Maximum number of CPUs to try to use at once. If not using sbatch, 
    # don't make this bigger than the number of available cores.
    max_cpus = 100 
    # Density for JPGs converted from PDFs
    jpg_density = 150

    # Specify the replicates and amplicons.
    replicates = ['PR8_replicate_1', 'PR8_replicate_2', 'PR8_replicate_3', 'Aichi68C_replicate_1', 'Aichi68C_replicate_2']
    amplicons = ['DNA', 'mutDNA', 'virus', 'mutvirus']

    # seed the random number generator used during inference of amino-acid preferences
    seed = 1

    # set which ``dms_tools`` scripts to run. These must be run in order. This script
    # is not smart enough to know if you've already run the required scripts in order,
    # so use with caution.
    run_inferprefs = True
    run_merge = True
    run_logoplot = True

    if run_inferprefs:
        processes = []

        for replicate in replicates:
            DNA = mapmuts_output_dir + '/%s/DNA/%s_DNA_codoncounts.txt' % (replicate, replicate)
            virus = mapmuts_output_dir + '/%s/virus/%s_virus_codoncounts.txt' % (replicate, replicate)
            mutDNA = mapmuts_output_dir + '/%s/mutDNA/%s_mutDNA_codoncounts.txt' % (replicate, replicate)
            mutvirus = mapmuts_output_dir + '/%s/mutvirus/%s_mutvirus_codoncounts.txt' % (replicate, replicate)
            outfile = dmstools_output_dir + '/%s_prefs.txt' % replicate
            commands = ['--chartype codon_to_aa', '--errpre %s' % DNA, '--errpost %s' % virus, '--excludestop', '--ncpus 6 --seed %d' % seed, mutDNA, mutvirus, outfile]
            processes.append(multiprocessing.Process(target=RunScript,\
                args=(dmstools_output_dir, "%s_inferprefs" % replicate, 'dms_inferprefs', commands, use_sbatch, 12),
                kwargs={'use_full_partition':True}))
        RunProcesses(processes, nmultiruns=max_cpus)


    if run_merge:
        processes = []

        # make average PR/1934 and average Aichi/1968 current study:
        for strain in ['PR8','Aichi68C']:
            outfile = dmstools_output_dir + '/%s_mean_prefs.txt' % strain
            infiles = ['%s/%s_prefs.txt' % (dmstools_output_dir, replicate) for replicate in replicates if strain in replicate]
            commands = ['--excludestop', outfile, 'average'] + infiles
            processes.append(multiprocessing.Process(target=RunScript,\
                args=(dmstools_output_dir, "%s_merge" % strain, 'dms_merge', commands, use_sbatch, 12)))

        # make stop-removed pref files from Aichi/1968 previous study:
        bloom2014replicates = ['replicate_A', 'replicate_B']
        bloom2014samples = ['WT-1', 'WT-2', 'N334H-1', 'N334H-2']
        bloom2014filesbasepath = previous_aichi_analysis_dir
        for bloom2014rep in bloom2014replicates:
            for bloom2014samp in bloom2014samples:
                pfile = "%s/%s/%s/p1_equilibriumpreferences.txt" % (bloom2014filesbasepath, bloom2014rep, bloom2014samp)
                if 'WT' in bloom2014samp:
                    newprefix = "Aichi68%s_replicate_%s" % (bloom2014rep[-1],bloom2014samp[-1])
                else: # the N334H replicates are now termed "3" and "4" with experiment A and B.
                    newprefix = "Aichi68%s_replicate_%s" % (bloom2014rep[-1], str(int(bloom2014samp[-1])+2))
                outfile = dmstools_output_dir + '/%s_prefs.txt' % newprefix
                commands = ['--excludestop', outfile, 'average', pfile]
                processes.append(multiprocessing.Process(target=RunScript,\
                    args=(dmstools_output_dir, "%s_merge" % newprefix, 'dms_merge', commands, use_sbatch, 12)))

        # make average Aichi/1968 previous study:
        aichi68_previous_study_replicates = ['Aichi68A_replicate_%d' % i for i in range(1,5)] + ['Aichi68B_replicate_%d' % i for i in range(1,5)]
        aichi68_previous_study_files = ['%s/%s_prefs.txt' % (dmstools_output_dir, rep) for rep in aichi68_previous_study_replicates]
        outfile = '%s/mean_Aichi68_previous_study_prefs.txt' % dmstools_output_dir
        commands = ['--excludestop', outfile, 'average'] + aichi68_previous_study_files
        processes.append(multiprocessing.Process(target=RunScript,\
            args=(dmstools_output_dir, "meanAichiPrevious_merge", 'dms_merge', commands, use_sbatch, 12)))

        # make average Aichi/1968 (previous study + current study)/2:
        aichi_mean_files = ['%s/mean_Aichi68_previous_study_prefs.txt' % dmstools_output_dir, '%s/Aichi68C_mean_prefs.txt' % dmstools_output_dir]
        outfile = '%s/mean_Aichi68_both_studies_prefs.txt' % dmstools_output_dir
        commands = ['--excludestop', outfile, 'average'] + aichi_mean_files
        processes.append(multiprocessing.Process(target=RunScript,\
            args=(dmstools_output_dir, "meanAichiBoth_merge", 'dms_merge', commands, use_sbatch, 12)))

        # make average across homologs (average Aichi/1968 + average PR/1934)/2:
        homolog_mean_files = ['%s/mean_Aichi68_both_studies_prefs.txt' % dmstools_output_dir, '%s/PR8_mean_prefs.txt' % dmstools_output_dir]
        outfile = '%s/mean_NP_both_homologs_prefs.txt' % dmstools_output_dir
        commands = ['--excludestop', outfile, 'average'] + homolog_mean_files
        processes.append(multiprocessing.Process(target=RunScript,\
            args=(dmstools_output_dir, "meanNP_bothHomologs_merge", 'dms_merge', commands, use_sbatch, 12)))

        # make stop-removed pref files from WSN-HA:
        WSN_files = ['%s/replicate_%s/replicate_%s_equilibriumpreferences.txt' % (previous_ha_analysis_dir, i, i) for i in [1,2,3]]
        for pfile, prefix in zip(WSN_files,['WSN_HA_rep1','WSN_HA_rep2','WSN_HA_rep3']):
            outfile = '%s/%s_prefs.txt' % (dmstools_output_dir, prefix)
            commands = ['--excludestop', outfile, 'average', pfile]
            processes.append(multiprocessing.Process(target=RunScript,\
                args=(dmstools_output_dir, "%s_merge" % prefix, 'dms_merge', commands, use_sbatch, 12)))

        # make average WSN-HA:
        outfile = '%s/WSN_HA_mean_prefs.txt' % (dmstools_output_dir)
        commands = ['--excludestop', outfile, 'average'] + WSN_files
        processes.append(multiprocessing.Process(target=RunScript,\
                args=(dmstools_output_dir, "WSNmean_merge", 'dms_merge', commands, use_sbatch, 12)))

        RunProcesses(processes, nmultiruns=max_cpus)


    if run_logoplot:

        processes = []

        
        # Make logoplots for sites 2-498. 
        # First make prefs files stripped of site 1, which was not mutagenized.
        sites = range(2,499)
        for f in ['PR8_mean_prefs.txt','mean_Aichi68_both_studies_prefs.txt','mean_NP_both_homologs_prefs.txt']:
            these_prefs = rmsdtools.ReadDMSToolsFormattedPrefsToMMFormattedDict(dmstools_output_dir+'/'+f)
            rmsdtools.WriteEquilFreqs(these_prefs, mapmuts.sequtils.AminoAcids(includestop=False), sites, "%s/%s_stripped"%(dmstools_output_dir,f))

        # make logoplot of mean PR/1934 preferences
        outfile = 'logoplot_mean_PR1934.pdf'
        infile = dmstools_output_dir + '/PR8_mean_prefs.txt_stripped'
        commands = [infile, outfile, '--nperline 56']
        processes.append(multiprocessing.Process(target=RunScript,\
                args=(dmstools_output_dir, "PR_logo", 'dms_logoplot', commands, use_sbatch, 12)))

        # make logoplot of mean Aichi/1968 preferences
        outfile = 'logoplot_mean_Aichi1968.pdf'
        infile = dmstools_output_dir + '/mean_Aichi68_both_studies_prefs.txt_stripped'
        commands = [infile, outfile, '--nperline 56']
        processes.append(multiprocessing.Process(target=RunScript,\
                args=(dmstools_output_dir, "Aichi_logo", 'dms_logoplot', commands, use_sbatch, 12)))

        # make logoplot of mean NP preferences across both homologs
        outfile = 'logoplot_meanNP_bothHomologs.pdf'
        infile = dmstools_output_dir + '/mean_NP_both_homologs_prefs.txt_stripped'
        commands = [infile, outfile, '--nperline 56']
        processes.append(multiprocessing.Process(target=RunScript,\
                args=(dmstools_output_dir, "mean_NP_bothHomologs_logo", 'dms_logoplot', commands, use_sbatch, 12)))
        
        # Make logoplots for the replicate measurements within each homolog for a subset of sites.
        # First make prefs files containing the replicate measurements at each site for each homolog.
        sites = [309, 298, 408, 470]
        PR8_replicate_files = sorted(glob.glob('%s/PR8_replicate_*_prefs.txt' % dmstools_output_dir))
        Aichi68_replicate_files = sorted(glob.glob('%s/Aichi68*_replicate_*_prefs.txt' % dmstools_output_dir))
        for prefix,filegroup in zip(['PR8','Aichi68'], [PR8_replicate_files, Aichi68_replicate_files]):
            rmsdtools.WriteReplicateMeasurementPrefs( filegroup, sites, outputdir = dmstools_output_dir, fileprefix = prefix )
            #saves a file like 'Aichi68_site_19_replicatemeasurements.txt'
        # Now make the logoplots
        for i,f in enumerate(glob.glob('%s/*_site_*_replicatemeasurements.txt' % dmstools_output_dir)):
            outfile = f[f.rfind('/')+1:].rstrip('.txt') + '.pdf'
            commands = [f,outfile]
            processes.append(multiprocessing.Process(target=RunScript,\
                args=(dmstools_output_dir, "replicate_measurements_logo_%d" % i, 'dms_logoplot', commands, use_sbatch, 12)))


        RunProcesses(processes, nmultiruns=max_cpus)


if __name__ == '__main__':
    main() # run the script